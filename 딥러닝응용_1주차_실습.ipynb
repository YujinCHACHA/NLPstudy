{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgUgcPZtW4ZSVrfiQdsP+j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YujinCHACHA/NLPstudy/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%91%EC%9A%A9_1%EC%A3%BC%EC%B0%A8_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시소러스\n",
        "- nltk(natural language)"
      ],
      "metadata": {
        "id": "RkVP3oJk_-5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T_OMN6eMRke",
        "outputId": "79b849a3-60af-4621-8e52-fca24b4af40a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uz-ynm__7bG",
        "outputId": "32478e39-a387-4e15-e429-fc6dfc13d50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet.synsets('car')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqXl3k5YAIh8",
        "outputId": "26d924f7-149b-439c-b4b2-04189af4db6d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01'),\n",
              " Synset('car.n.02'),\n",
              " Synset('car.n.03'),\n",
              " Synset('car.n.04'),\n",
              " Synset('cable_car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어의 뜻이 여러 개이기 때문에 car.n.01과 같이 넘버가 붙는다."
      ],
      "metadata": {
        "id": "fI1VgGVlN2c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car =wordnet.synset('car.n.01')"
      ],
      "metadata": {
        "id": "aGSpEvXZAQn9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "car.definition() # car의 첫 번째 정의"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GjihAZkMNSrs",
        "outputId": "1c4dd950-6de1-43da-9893-2fc0b3cf923c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "car.lemma_names() # car의 유의어"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfJNwP6PNjmk",
        "outputId": "68e6819c-b1af-48bc-b267-7eb5db0a3a0d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['car', 'auto', 'automobile', 'machine', 'motorcar']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "car.hypernym_paths()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO8pUJqUQV0P",
        "outputId": "6584d9c7-835e-47c8-fdb0-1287ff88ad7d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Synset('entity.n.01'),\n",
              "  Synset('physical_entity.n.01'),\n",
              "  Synset('object.n.01'),\n",
              "  Synset('whole.n.02'),\n",
              "  Synset('artifact.n.01'),\n",
              "  Synset('instrumentality.n.03'),\n",
              "  Synset('container.n.01'),\n",
              "  Synset('wheeled_vehicle.n.01'),\n",
              "  Synset('self-propelled_vehicle.n.01'),\n",
              "  Synset('motor_vehicle.n.01'),\n",
              "  Synset('car.n.01')],\n",
              " [Synset('entity.n.01'),\n",
              "  Synset('physical_entity.n.01'),\n",
              "  Synset('object.n.01'),\n",
              "  Synset('whole.n.02'),\n",
              "  Synset('artifact.n.01'),\n",
              "  Synset('instrumentality.n.03'),\n",
              "  Synset('conveyance.n.03'),\n",
              "  Synset('vehicle.n.01'),\n",
              "  Synset('wheeled_vehicle.n.01'),\n",
              "  Synset('self-propelled_vehicle.n.01'),\n",
              "  Synset('motor_vehicle.n.01'),\n",
              "  Synset('car.n.01')]]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "novel = wordnet.synset('novel.n.01')"
      ],
      "metadata": {
        "id": "PJMXutwmOWgA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog = wordnet.synset('dog.n.01')"
      ],
      "metadata": {
        "id": "nrGU1pyAO2G8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "motorcycle = wordnet.synset('motorcycle.n.01')"
      ],
      "metadata": {
        "id": "EEjHc0_0O46k"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Vm6yy2tO9QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess()"
      ],
      "metadata": {
        "id": "FHV9MH6HfiNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text = 'You say goodbye and I say hello.'"
      ],
      "metadata": {
        "id": "tInBRkQNgRDe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower() # 소문자로 바꿔주는 함수\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "edZizWZ1gSrC",
        "outputId": "826c09fe-b333-4272-8f34-fbec3c0f864a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you say goodbye and i say hello.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.replace('.',' .') # 문자열 대체\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GPZUF--TgUrj",
        "outputId": "334a8b8e-71a2-40cf-eafc-36f54f96926e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you say goodbye and i say hello .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split(' ') # 띄어쓰기 기준으로 문자열 분할 후 리스트에 하나씩 담기"
      ],
      "metadata": {
        "id": "9tXVR5AEgayp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBjLglPqgd-u",
        "outputId": "b750f5dc-6dc0-4816-80ed-627bbf75f57f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_by_a = text.split('a')\n",
        "words_by_a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtVdlsbTgqZR",
        "outputId": "5103a798-a92d-4e0f-8929-69e723c169f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you s', 'y goodbye ', 'nd i s', 'y hello .']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id = {}\n",
        "id_to_word = {}"
      ],
      "metadata": {
        "id": "quQWkgrnhCFh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  if word not in word_to_id:\n",
        "    new_id = len(word_to_id)\n",
        "    word_to_id[word] = new_id\n",
        "    id_to_word[new_id] = word"
      ],
      "metadata": {
        "id": "cMgWFQhFhJWi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_to_id)\n",
        "print(id_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4J5cndCjTHS",
        "outputId": "c4999bb4-79c4-4da5-fc19-57384f1a52c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = np.array([word_to_id[w] for w in words])"
      ],
      "metadata": {
        "id": "jHwojWQFjZh9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtKC2rhGj4Pl",
        "outputId": "26aca2f5-de84-4dea-8949-30e6b7a46ac6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 1 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word"
      ],
      "metadata": {
        "id": "88eARbfjj-PH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '아무 북이 있어야 하잖아요? 우리 말로 이해하기 위해선 사전이 있어야 하잖아요'\n",
        "preprocess(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UjSzFJYkwqC",
        "outputId": "445dbd77-2fcd-4704-c7fc-f5387484b0b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 2, 9]),\n",
              " {'아무': 0,\n",
              "  '북이': 1,\n",
              "  '있어야': 2,\n",
              "  '하잖아요?': 3,\n",
              "  '우리': 4,\n",
              "  '말로': 5,\n",
              "  '이해하기': 6,\n",
              "  '위해선': 7,\n",
              "  '사전이': 8,\n",
              "  '하잖아요': 9},\n",
              " {0: '아무',\n",
              "  1: '북이',\n",
              "  2: '있어야',\n",
              "  3: '하잖아요?',\n",
              "  4: '우리',\n",
              "  5: '말로',\n",
              "  6: '이해하기',\n",
              "  7: '위해선',\n",
              "  8: '사전이',\n",
              "  9: '하잖아요'})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word"
      ],
      "metadata": {
        "id": "1dQcoW_TfpYJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTadF_KofpTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import os\n",
        "from common.np import *\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    '''코사인 유사도 산출\n",
        "\n",
        "    :param x: 벡터\n",
        "    :param y: 벡터\n",
        "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
        "    :return:\n",
        "    '''\n",
        "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    '''유사 단어 검색\n",
        "\n",
        "    :param query: 쿼리(텍스트)\n",
        "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
        "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
        "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
        "    :param top: 상위 몇 개까지 출력할 지 지정\n",
        "    '''\n",
        "    if query not in word_to_id:\n",
        "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    # 코사인 유사도 계산\n",
        "    vocab_size = len(id_to_word)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def convert_one_hot(corpus, vocab_size):\n",
        "    '''원핫 표현으로 변환\n",
        "\n",
        "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
        "    :param vocab_size: 어휘 수\n",
        "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
        "    '''\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "    '''동시발생 행렬 생성\n",
        "\n",
        "    :param corpus: 말뭉치(단어 ID 목록)\n",
        "    :param vocab_size: 어휘 수\n",
        "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
        "    :return: 동시발생 행렬\n",
        "    '''\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_idx = idx - i\n",
        "            right_idx = idx + i\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix\n",
        "\n",
        "\n",
        "def ppmi(C, verbose=False, eps = 1e-8):\n",
        "    '''PPMI(점별 상호정보량) 생성\n",
        "\n",
        "    :param C: 동시발생 행렬\n",
        "    :param verbose: 진행 상황을 출력할지 여부\n",
        "    :return:\n",
        "    '''\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100 + 1) == 0:\n",
        "                    print('%.1f%% 완료' % (100*cnt/total))\n",
        "    return M\n",
        "\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    '''맥락과 타깃 생성\n",
        "\n",
        "    :param corpus: 말뭉치(단어 ID 목록)\n",
        "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
        "    :return:\n",
        "    '''\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)\n",
        "\n",
        "\n",
        "def to_cpu(x):\n",
        "    import numpy\n",
        "    if type(x) == numpy.ndarray:\n",
        "        return x\n",
        "    return np.asnumpy(x)\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    import cupy\n",
        "    if type(x) == cupy.ndarray:\n",
        "        return x\n",
        "    return cupy.asarray(x)\n",
        "\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(grad ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "    if rate < 1:\n",
        "        for grad in grads:\n",
        "            grad *= rate\n",
        "\n",
        "\n",
        "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
        "    print('퍼플렉서티 평가 중 ...')\n",
        "    corpus_size = len(corpus)\n",
        "    total_loss, loss_cnt = 0, 0\n",
        "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
        "    jump = (corpus_size - 1) // batch_size\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        time_offset = iters * time_size\n",
        "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
        "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
        "\n",
        "        try:\n",
        "            loss = model.forward(xs, ts, train_flg=False)\n",
        "        except TypeError:\n",
        "            loss = model.forward(xs, ts)\n",
        "        total_loss += loss\n",
        "\n",
        "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print('')\n",
        "    ppl = np.exp(total_loss / max_iters)\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def eval_seq2seq(model, question, correct, id_to_char,\n",
        "                 verbos=False, is_reverse=False):\n",
        "    correct = correct.flatten()\n",
        "    # 머릿글자\n",
        "    start_id = correct[0]\n",
        "    correct = correct[1:]\n",
        "    guess = model.generate(question, start_id, len(correct))\n",
        "\n",
        "    # 문자열로 변환\n",
        "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
        "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
        "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
        "\n",
        "    if verbos:\n",
        "        if is_reverse:\n",
        "            question = question[::-1]\n",
        "\n",
        "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
        "        print('Q', question)\n",
        "        print('T', correct)\n",
        "\n",
        "        is_windows = os.name == 'nt'\n",
        "\n",
        "        if correct == guess:\n",
        "            mark = colors['ok'] + '☑' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'O'\n",
        "            print(mark + ' ' + guess)\n",
        "        else:\n",
        "            mark = colors['fail'] + '☒' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'X'\n",
        "            print(mark + ' ' + guess)\n",
        "        print('---')\n",
        "\n",
        "    return 1 if guess == correct else 0\n",
        "\n",
        "\n",
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    query_vec = normalize(query_vec)\n",
        "\n",
        "    similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "    if answer is not None:\n",
        "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if np.isnan(similarity[i]):\n",
        "            continue\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    if x.ndim == 2:\n",
        "        s = np.sqrt((x * x).sum(1))\n",
        "        x /= s.reshape((s.shape[0], 1))\n",
        "    elif x.ndim == 1:\n",
        "        s = np.sqrt((x * x).sum())\n",
        "        x /= s\n",
        "    return x"
      ],
      "metadata": {
        "id": "QhItEF9Tfjfc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}